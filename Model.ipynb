{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuM6K4QXxuNO"
      },
      "source": [
        "# **Import Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XJQsD08qxkU9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiI82I9nxxut"
      },
      "source": [
        "# **DarkNet-53**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jwXqX6Mfx406"
      },
      "outputs": [],
      "source": [
        "# Define the characteristics of a convolutional block in our CNN\n",
        "class ConvUnit(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(ConvUnit, self).__init__()\n",
        "    # create convolutional layer, bias = False becasue we're using batch normalisation and that has its own bias \n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "    # batch normalisation - stabalise training\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    # leaky relu - this should avoid vanishing gradients (alongside our skip connections) and also address the dying neuron problem \n",
        "    # 0.1 defines the negative slope in the ReLU function that will allow some negative weights not to evaluate to zero\n",
        "    self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    pass x (our input) through conv -> batch_norm - > ReLU\n",
        "    '''\n",
        "    return self.leaky_relu(self.bn(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NAdT8bAL0pt_"
      },
      "outputs": [],
      "source": [
        "# Define the characteristics of a residual unit in our CNN\n",
        "class ResidualUnit(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(ResidualUnit, self).__init__()\n",
        "    # first convolution with n=output_channels 1*1 kernels yielding img*img*out_channels feature map\n",
        "    # (downsample while retaining features)\n",
        "    self.conv1 = ConvUnit(in_channels, in_channels//2, kernel_size=1, stride=1, padding=0)\n",
        "    # second convolution (extract features)\n",
        "    self.conv2 = ConvUnit(in_channels//2, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    pass x through these convolutional layers and return original input + processed output\n",
        "    '''\n",
        "    # skip connection\n",
        "    return x + self.conv2(self.conv1(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZzmrT75l0wxf"
      },
      "outputs": [],
      "source": [
        "# Outline the CNN itself\n",
        "\n",
        "class DarkNet53(nn.Module):\n",
        "  def __init_(self):\n",
        "    super(DarkNet53, self).__init__()\n",
        "    # First convolutional layers (1) + batch norm and ReLU for each conv unit \n",
        "    self.conv1 = ConvUnit(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # Downsample (reduce dimensionality) (1 layer)\n",
        "    self.conv2 = ConvUnit(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # First residual unit + skip connection (2 convolutional layers)\n",
        "    self.res1 = self.make_layer(ResidualUnit(64, 1))\n",
        "\n",
        "    # process repeated to capture more complex features\n",
        "    self.conv3 = ConvUnit(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res2 = self.make_layer(ResidualUnit(128, 2)) # 4 layers\n",
        "    self.conv4 = ConvUnit(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res3 = self.make_layer(ResidualUnit(256, 8)) # 16 layers\n",
        "    self.conv5 = ConvUnit(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res4 = self.make_layer(ResidualUnit(512, 8)) # 16 layers\n",
        "    self.conv6 = ConvUnit(in_channels=512, out_channels=1024, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res5 = self.make_layer(ResidualUnit(1024, 4)) # 8 layers\n",
        "\n",
        "    # apply gloval average pooling to final feature map as per DarkNet-53 architecture (1 layer)\n",
        "    # final downsample before we feed the output feature map to the YOLO Layers (1*1 convolutional layers to predict the bounding boxes)\n",
        "    # yolo v1 used fully connected layers but in this v3 implementation 1*1 convolutional layers are used to directly predict boxes i.e we don't flatten the feature map\n",
        "    self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    # for a total of 53 layers ... what a surprise \n",
        "\n",
        "  def make_layer(self, unit, out_channels, num_units):\n",
        "    layers=[]\n",
        "    for _ in range(num_units):\n",
        "      layers.append(unit(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    feed input through the layers \n",
        "    '''\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.res1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.res2(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.res3(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.res4(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.res5(x)\n",
        "\n",
        "    x = self.global_avg_pool(x)\n",
        "    return x  # our feature map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNu8B1P-4nFH"
      },
      "source": [
        "#**\"YOLO Layer\"**\n",
        "\n",
        "After our Darknet53 CNN extracts the features, we use the 'yolo,' layer to complete object detection and classification in a single forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_eWk1oS5OIe"
      },
      "outputs": [],
      "source": [
        "class YOLOLayer(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes, anchors, input_dimension):\n",
        "    super(YOLOLayer, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_anchors = len(anchors)\n",
        "    self.input_dim = input_dimension\n",
        "    # Output layer\n",
        "    # convolutional layer convolving the DarkNet output witht a 1*1 kernel yielding c channels where c=(self.num_anchors * (5 + num_classes)\n",
        "    # our grid is defined as each cell in the final downsampled feature map (after global pooling in darknet)\n",
        "    # Each cell in the grid corresponds to a region of the original image and each cell is responsible for detecting objects within this region \n",
        "\n",
        "    # Each cell has 3 anchors (predefined sizes objects should be approximately at different aspect ratios) YOLO \n",
        "    # adjusts these anchors to define bounding boxes rather than drawing boxes from scratch so for each anchor \n",
        "    # we want (x, y, w, h) describing how much to offset each aspect of the anchor to surround the image, we want our confidence score that \n",
        "    # an object exists and we want class probabilities [x,y,w,h,conf,classes] per anchor, hence c=(self.num_anchors * (5 + num_classes)\n",
        "\n",
        "    \n",
        "    self.conv = nn.Conv2d(in_channels, self.num_anchors * (5 + num_classes), kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, _, grid_size, _ = x.shape\n",
        "    \n",
        "    # This outputs a 4D tensor of shape (batch_size, (anchors*(5+classes), grid_size, grid_size)) - a hypercube of data (not relevant, just like the word 'hypercube')\n",
        "    # we use .view() to transform to a 5D tensor of shape (batch_size, anchors, (x,y,w,h,conf,2 class probs =7) grid, grid)\n",
        "    # This way each anchor corresponds to the ReLU activations we are attributing to that anchor and that will be learned by the model \n",
        "    prediction = self.conv(x).view(batch_size, self.num_anchors, 5 + self.num_classes, grid_size, grid_size)\n",
        "\n",
        "    # this changes the order now to shape = (batch, anchors, grid, grid, (x,y,w,h,conf,2 class probs =7))\n",
        "    # this allows us to cleanly access a given anchor at a given cell and output its downsampled ReLU activations from the CNN \n",
        "    # these activations initially have nothing to do with x,y,w,h,conf,class probs, they are just a downsampling from the output of the CNN\n",
        "    # but the model will learn to optimise them essentially outputting what we want where we want through the loss function\n",
        "    prediction = prediction.premute(0, 1, 3, 4, 2).contiguous() \n",
        "\n",
        "    # Now we have to take these predictions and convert the raw ReLU activations into our bounded boxes \n",
        "    # I've written the formulae here : \n",
        "    #      x = (sigmoid(predicted x offset) + grid cell x) / grid size) \n",
        "    #      y = (sigmoid(predicted y offset) + grid cell y) / grid size) \n",
        "    #      w = anchor w * exp(predicted w offset) / input_width\n",
        "    #      h = anchor h * exp(predicted h offset) / input_height\n",
        "    #      conf = sigmoid(predicted)\n",
        "    #      class probs = softmax/sigmoid(predicted) depending on how many \n",
        "\n",
        "    # the grid is sort of imaginary its just the size of the downsampled tensor minus the channels obviously and we can map it to the original image\n",
        "\n",
        "    x = prediction[..., 0] # get x for every anchor for every cell for every image in batch \n",
        "    y = prediction[..., 1] # get y for every anchor for every cell for every image in batch \n",
        "    w = prediction[..., 2] # get w for every anchor for every cell for every image in batch \n",
        "    h = prediction[..., 3] # get h for every anchor for every cell for every image in batch \n",
        "    \n",
        "    conf = torch.sigmoid(prediction[..., 4]) # sigmoid of every conf prediction in every anchor in every cell in batch\n",
        "    class_probs = torch.sigmoid(prediction[..., 5:], dim=-1) # we only have two classes so I went for sigmoid \n",
        "\n",
        "    # grid_x and grid_y are tensors that store every x and every y coordinate respectively of the grid cells\n",
        "    # meshgrid() creates a grid of grid_size * grid_size and assigns coordinates to grid_x and grid_y\n",
        "    grid_x, grid_y = torch.meshgrid(torch.arrange(grid_size), torch.arrange(grid_size), indexing='ij')\n",
        "    # these should be tensor.int64 by default but they will be involved in floating point calculations in a matter of 3 lines so \n",
        "    # best to convert them to floats so python doesn't start nagging us \n",
        "    grid_x, grid_y = grid_x.float(), grid_y.float()\n",
        "\n",
        "    # now the actual bounded box calculations according to fomulae outlined above \n",
        "    bx = (torch.sigmoid(x) + grid_x) / grid_size # grid_X and Y ensure bounding box mapped to right position and \n",
        "    by = (torch.sigmoid(y) + grid_y) / grid_size # sigmoid helps the bounding box centre stay in the cell \n",
        "\n",
        "    # I feel like this looks more complicated than it is - its just the formula as above but \n",
        "    # we use [:, 0/1] to index anchors from the anchors outlined in the YOLOv3 class and .view(1,-1,1,1)\n",
        "    # to match the shape of w which has batch, grid and anchors so in order to match we transform it to shape \n",
        "    # (1, num_anchors, 1, 1) - [[[[num_anchors]]]] * exp(w) \n",
        "    # exp() is used becasue YOLO doesnt predict width it predicts a logarithmic offset of the predefined anchor so exp makes it stay positive \n",
        "    # and scale by a meaningful amount \n",
        "    # lastly dividing it by the input_dimension of the image gives us a decimal that we can use to adjust the boxes in a scale \n",
        "    # invariant way  \n",
        "    bw = self.anchors[:, 0].view(1, -1, 1, 1) * torch.exp(w) / self.input_dim\n",
        "    bh = self.anchors[:, 1].view(1, -1, 1, 1) * torch.exp(h) / self.input_dim\n",
        "\n",
        "    # stack all predictions into a single tensor \n",
        "    boxes = torch.stack((bx, by, bw, bh, conf), dim=-1)\n",
        "\n",
        "    return boxes, class_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMUsXaGb6v8b"
      },
      "source": [
        "# **YOLOv3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BbLEKORT60wf"
      },
      "outputs": [],
      "source": [
        "class YOLOv3(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(YOLOv3, self).__init__()\n",
        "    self.base = DarkNet53()\n",
        "    self.yolo1 = YOLOLayer(1024, num_classes, anchors=[(116, 90), (156, 198), (373, 326)])\n",
        "    self.yolo2 = YOLOLayer(512, num_classes, anchors=[(30, 61), (62, 45), (59, 119)])\n",
        "    self.yolo3 = YOLOLayer(256, num_classes, anchors=[(10, 13), (16, 30), (33, 23)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    We pass x - through darknet extracting feature maps at multiple scales \n",
        "\n",
        "    Three YOLO Layers are implemented to detect objects of variable size \n",
        "    - Layer 1 -> deep map -> large objects \n",
        "    - Layer 2 -> mid map -> medium objects \n",
        "    - Layer 3 -> shallow map -> small objects \n",
        "\n",
        "    We later apply NMS to remove duplicates ( boxes with >= 40% overlap )\n",
        "    '''\n",
        "    x = self.base(x)\n",
        "    return self.yolo1(x), self.yolo2(x), self.yolo3(x) # (Three detection heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OctUQa58Mgk"
      },
      "source": [
        "# **Loss Function**\n",
        "\n",
        "Three phase, Squared Error Approach\n",
        "\n",
        "1) Bounding box error - heavy penalty\n",
        "\n",
        "2) Incorrect object detected in cell\n",
        "\n",
        "3) Error between prediction and target prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NCMH3G1o8nNN"
      },
      "outputs": [],
      "source": [
        "class YOLOLoss(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(YOLOLoss, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.mse = nn.MSELoss()\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "    self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, predictions, targets):\n",
        "    # 1 : loss for bounding box (x,y coords , width, height)\n",
        "    box_loss = self.mse(predictions[..., :4], targets[..., :4])\n",
        "\n",
        "    # 2 : object confidence\n",
        "    conf_loss = self.bce(predictions[..., 4], targets[..., 4])\n",
        "\n",
        "    # 3 : class predictions\n",
        "    class_loss = self.ce(predictions[..., 5:], targets[..., 5:].argmax(-1))\n",
        "\n",
        "    total_loss = box_loss + conf_loss + class_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0bwC26Mp8AYX"
      },
      "outputs": [],
      "source": [
        "# model = YOLOv3(num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non Maximum Suppression\n",
        "\n",
        "We're allowing 3 anchors per cell as per YOLOv3 so there's lots of duplicates to remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nonMaxSuppression(predictions, conf_threshold=0.5, iou_threshold=0.4):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    pass \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
