{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuM6K4QXxuNO"
      },
      "source": [
        "# **Import Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XJQsD08qxkU9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIJK4MhBOjFs",
        "outputId": "665183e1-e2ac-4e35-c6b9-708f4b26d0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.58)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n"
          ]
        }
      ],
      "source": [
        "pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow"
      ],
      "metadata": {
        "id": "0h9h9_tLPfgF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQCeosw3OjFs",
        "outputId": "bd86865f-6a8e-4c47-9dfc-ffd942051be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Fire-Smoke-Detection-Yolov11-2 to yolov11:: 100%|██████████| 581927/581927 [00:33<00:00, 17590.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Fire-Smoke-Detection-Yolov11-2 in yolov11:: 100%|██████████| 20938/20938 [00:03<00:00, 6957.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# rf = Roboflow(api_key=\"gMu18twOPMgwp22tEQfY\")\n",
        "# project = rf.workspace(\"sayed-gamall\").project(\"fire-smoke-detection-yolov11\")\n",
        "# version = project.version(2)\n",
        "# dataset = version.download(\"darknet\")\n",
        "\n",
        "rf = Roboflow(api_key=\"gMu18twOPMgwp22tEQfY\")\n",
        "project = rf.workspace(\"sayed-gamall\").project(\"fire-smoke-detection-yolov11\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "\n",
        "# rf = Roboflow(api_key=\"gMu18twOPMgwp22tEQfY\")\n",
        "# project = rf.workspace(\"sayed-gamall\").project(\"fire-smoke-detection-yolov11\")\n",
        "# version = project.version(2)\n",
        "# dataset = version.download(\"yolov4pytorch\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiI82I9nxxut"
      },
      "source": [
        "# **DarkNet-53**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jwXqX6Mfx406"
      },
      "outputs": [],
      "source": [
        "# Define the characteristics of a convolutional block in our CNN\n",
        "class ConvUnit(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(ConvUnit, self).__init__()\n",
        "    # create convolutional layer, bias = False becasue we're using batch normalisation and that has its own bias\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "    # batch normalisation - stabalise training\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    # leaky relu - this should avoid vanishing gradients (alongside our skip connections) and also address the dying neuron problem\n",
        "    # 0.1 defines the negative slope in the ReLU function that will allow some negative weights not to evaluate to zero\n",
        "    self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    pass x (our input) through conv -> batch_norm - > ReLU\n",
        "    '''\n",
        "    return self.leaky_relu(self.bn(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NAdT8bAL0pt_"
      },
      "outputs": [],
      "source": [
        "# Define the characteristics of a residual unit in our CNN\n",
        "class ResidualUnit(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(ResidualUnit, self).__init__()\n",
        "    # first convolution with n=output_channels 1*1 kernels yielding img*img*out_channels feature map\n",
        "    # (downsample while retaining features)\n",
        "    self.conv1 = ConvUnit(in_channels, in_channels//2, kernel_size=1, stride=1, padding=0)\n",
        "    # second convolution (extract features)\n",
        "    self.conv2 = ConvUnit(in_channels//2, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    pass x through these convolutional layers and return original input + processed output\n",
        "    '''\n",
        "    # skip connection\n",
        "    return x + self.conv2(self.conv1(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZzmrT75l0wxf"
      },
      "outputs": [],
      "source": [
        "# Outline the CNN itself\n",
        "\n",
        "class DarkNet53(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DarkNet53, self).__init__()\n",
        "    # First convolutional layers (1) + batch norm and ReLU for each conv unit\n",
        "    self.conv1 = ConvUnit(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # Downsample (reduce dimensionality) (1 layer)\n",
        "    self.conv2 = ConvUnit(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # First residual unit + skip connection (2 convolutional layers)\n",
        "    self.res1 = self.make_layer(ResidualUnit, 64, 1)\n",
        "\n",
        "    # process repeated to capture more complex features\n",
        "    self.conv3 = ConvUnit(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res2 = self.make_layer(ResidualUnit, 128, 2) # 4 layers\n",
        "    self.conv4 = ConvUnit(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res3 = self.make_layer(ResidualUnit, 256, 8) # 16 layers\n",
        "    self.conv5 = ConvUnit(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res4 = self.make_layer(ResidualUnit, 512, 8) # 16 layers\n",
        "    self.conv6 = ConvUnit(in_channels=512, out_channels=1024, kernel_size=3, stride=2, padding=1) # 1 layer\n",
        "    self.res5 = self.make_layer(ResidualUnit, 1024, 4) # 8 layers\n",
        "\n",
        "    # apply gloval average pooling to final feature map as per DarkNet-53 architecture (1 layer)\n",
        "    # final downsample before we feed the output feature map to the YOLO Layers (1*1 convolutional layers to predict the bounding boxes)\n",
        "    # yolo v1 used fully connected layers but in this v3 implementation 1*1 convolutional layers are used to directly predict boxes i.e we don't flatten the feature map\n",
        "    self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    # for a total of 53 layers ... what a surprise\n",
        "\n",
        "  def make_layer(self, unit, out_channels, num_units):\n",
        "    layers=[]\n",
        "    for _ in range(num_units):\n",
        "      layers.append(unit(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    feed input through the layers\n",
        "    '''\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.res1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.res2(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.res3(x)\n",
        "    c1=x\n",
        "    x = self.conv5(x)\n",
        "    x = self.res4(x)\n",
        "    c2=x\n",
        "    x = self.conv6(x)\n",
        "    x = self.res5(x)\n",
        "    c3=x\n",
        "\n",
        "    # x = self.global_avg_pool(x) -> Part of DarkNet-53 but not used in YOLO V3\n",
        "    # we take the raw convolutional outputs as below for multi scale detection\n",
        "    return c1, c2, c3 # our feature map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNu8B1P-4nFH"
      },
      "source": [
        "#**\"YOLO Layer\"**\n",
        "\n",
        "After our Darknet53 CNN extracts the features, we use the 'yolo,' layer to complete object detection and classification in a single forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g_eWk1oS5OIe"
      },
      "outputs": [],
      "source": [
        "class YOLOLayer(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes, anchors, input_dimension):\n",
        "    super(YOLOLayer, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_anchors = len(anchors)\n",
        "    self.input_dim = input_dimension\n",
        "\n",
        "    self.anchors = torch.tensor(anchors, dtype=torch.float32)\n",
        "    # Output layer\n",
        "    # convolutional layer convolving the DarkNet output witht a 1*1 kernel yielding c channels where c=(self.num_anchors * (5 + num_classes)\n",
        "    # our grid is defined as each cell in the final downsampled feature map (after global pooling in darknet)\n",
        "    # Each cell in the grid corresponds to a region of the original image and each cell is responsible for detecting objects within this region\n",
        "\n",
        "    # Each cell has 3 anchors (predefined sizes objects should be approximately at different aspect ratios) YOLO\n",
        "    # adjusts these anchors to define bounding boxes rather than drawing boxes from scratch so for each anchor\n",
        "    # we want (x, y, w, h) describing how much to offset each aspect of the anchor to surround the image, we want our confidence score that\n",
        "    # an object exists and we want class probabilities [x,y,w,h,conf,classes] per anchor, hence c=(self.num_anchors * (5 + num_classes)\n",
        "\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels, self.num_anchors * (5 + num_classes), kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, _, grid_size, _ = x.shape\n",
        "\n",
        "    anchors = self.anchors.to(x.device)\n",
        "\n",
        "    # This outputs a 4D tensor of shape (batch_size, (anchors*(5+classes), grid_size, grid_size)) - a hypercube of data (not relevant, just like the word 'hypercube')\n",
        "    # we use .view() to transform to a 5D tensor of shape (batch_size, anchors, (x,y,w,h,conf,2 class probs =7) grid, grid)\n",
        "    # This way each anchor corresponds to the ReLU activations we are attributing to that anchor and that will be learned by the model\n",
        "    prediction = self.conv(x).view(batch_size, self.num_anchors, 5 + self.num_classes, grid_size, grid_size)\n",
        "\n",
        "    # this changes the order now to shape = (batch, anchors, grid, grid, (x,y,w,h,conf,2 class probs =7))\n",
        "    # this allows us to cleanly access a given anchor at a given cell and output its downsampled ReLU activations from the CNN\n",
        "    # these activations initially have nothing to do with x,y,w,h,conf,class probs, they are just a downsampling from the output of the CNN\n",
        "    # but the model will learn to optimise them essentially outputting what we want where we want through the loss function\n",
        "    prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "    # Now we have to take these predictions and convert the raw ReLU activations into our bounded boxes\n",
        "    # I've written the formulae here :\n",
        "    #      x = (sigmoid(predicted x offset) + grid cell x) / grid size)\n",
        "    #      y = (sigmoid(predicted y offset) + grid cell y) / grid size)\n",
        "    #      w = anchor w * exp(predicted w offset) / input_width\n",
        "    #      h = anchor h * exp(predicted h offset) / input_height\n",
        "    #      conf = sigmoid(predicted)\n",
        "    #      class probs = softmax/sigmoid(predicted) depending on how many\n",
        "\n",
        "    # the grid is sort of imaginary its just the size of the downsampled tensor (minus the channels obviously) and we can map it to the original image\n",
        "\n",
        "    x = prediction[..., 0] # get x for every anchor for every cell for every image in batch\n",
        "    y = prediction[..., 1] # get y for every anchor for every cell for every image in batch\n",
        "    w = prediction[..., 2] # get w for every anchor for every cell for every image in batch\n",
        "    h = prediction[..., 3] # get h for every anchor for every cell for every image in batch\n",
        "\n",
        "    conf = torch.sigmoid(prediction[..., 4]) # sigmoid of every conf prediction in every anchor in every cell in batch\n",
        "    class_probs = torch.sigmoid(prediction[..., 5:]) # we only have two classes so I went for sigmoid\n",
        "\n",
        "    # grid_x and grid_y are tensors that store every x and every y coordinate respectively of the grid cells\n",
        "    # meshgrid() creates a grid of grid_size * grid_size and assigns coordinates to grid_x and grid_y\n",
        "    grid_x, grid_y = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size), indexing='ij')\n",
        "    # these should be tensor.int64 by default but they will be involved in floating point calculations in a matter of 3 lines so\n",
        "    # best to convert them to floats so python doesn't start nagging us\n",
        "    grid_x, grid_y = grid_x.float().to(x.device), grid_y.float().to(x.device)\n",
        "\n",
        "    # now the actual bounded box calculations according to fomulae outlined above\n",
        "    bx = (torch.sigmoid(x) + grid_x) / grid_size # grid_X and Y ensure bounding box mapped to right position and\n",
        "    by = (torch.sigmoid(y) + grid_y) / grid_size # sigmoid helps the bounding box centre stay in the cell\n",
        "\n",
        "    # I feel like this looks more complicated than it is - its just the formula as above but\n",
        "    # we use [:, 0/1] to index anchors from the anchors outlined in the YOLOv3 class and .view(1,-1,1,1)\n",
        "    # to match the shape of w which has batch, grid and anchors so in order to match we transform it to shape\n",
        "    # (1, num_anchors, 1, 1) - [[[[num_anchors]]]] * exp(w)\n",
        "    # exp() is used becasue YOLO doesnt predict width it predicts a logarithmic offset of the predefined anchor so exp makes it stay positive\n",
        "    # and scale by a meaningful amount\n",
        "    # lastly dividing it by the input_dimension of the image gives us a decimal that we can use to adjust the boxes in a scale\n",
        "    # invariant way\n",
        "    bw = anchors[:, 0].view(1, -1, 1, 1) * torch.exp(w) / self.input_dim\n",
        "    bh = anchors[:, 1].view(1, -1, 1, 1) * torch.exp(h) / self.input_dim\n",
        "\n",
        "    # stack all predictions into a single tensor\n",
        "    boxes = torch.stack((bx, by, bw, bh, conf), dim=-1)\n",
        "\n",
        "    return boxes, class_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMUsXaGb6v8b"
      },
      "source": [
        "# **YOLOv3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "BbLEKORT60wf"
      },
      "outputs": [],
      "source": [
        "class YOLOv3(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(YOLOv3, self).__init__()\n",
        "    self.base = DarkNet53()\n",
        "\n",
        "    self.yolo1 = YOLOLayer(1024, num_classes, anchors=[(116, 90), (156, 198), (373, 326)], input_dimension=604)\n",
        "    self.yolo2 = YOLOLayer(512, num_classes, anchors=[(30, 61), (62, 45), (59, 119)], input_dimension=604)\n",
        "    self.yolo3 = YOLOLayer(256, num_classes, anchors=[(10, 13), (16, 30), (33, 23)], input_dimension=604)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    We pass x - through darknet extracting feature maps at multiple scales\n",
        "\n",
        "    Three YOLO Layers are implemented to detect objects of variable size\n",
        "    - Layer 1 -> deep map -> large objects\n",
        "    - Layer 2 -> mid map -> medium objects\n",
        "    - Layer 3 -> shallow map -> small objects\n",
        "\n",
        "    We later apply NMS to remove duplicates ( boxes with >= 40% overlap )\n",
        "    '''\n",
        "    c1, c2, c3 = self.base(x) # Three depths from three stages of CNN\n",
        "    out1 = self.yolo1(c3)\n",
        "    out2 = self.yolo2(c2)\n",
        "    out3 = self.yolo3(c1)\n",
        "    return out1, out2, out3 # (Three detection heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OctUQa58Mgk"
      },
      "source": [
        "# **Loss Function**\n",
        "\n",
        "Three phase, Squared Error Approach\n",
        "\n",
        "1) Bounding box error - heavy penalty\n",
        "\n",
        "2) Incorrect object detected in cell\n",
        "\n",
        "3) Error between prediction and target prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "NCMH3G1o8nNN"
      },
      "outputs": [],
      "source": [
        "class YOLOLoss(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(YOLOLoss, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.mse = nn.MSELoss()\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "    self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, predictions, targets):\n",
        "\n",
        "    pred_boxes = predictions[..., 1:5] # (bx, by, bw, bh)\n",
        "    target_boxes = targets[..., 1:5] # (bx, by, bw, bh)(target)\n",
        "    pred_conf = predictions[..., 0]\n",
        "    target_conf = targets[..., 0]\n",
        "    # pred_classes = predictions[..., 5:] - not present in annotations\n",
        "    # target_classes = targets[..., 5:]\n",
        "\n",
        "    # 1 : loss for bounding box (x,y coords) i.e the position of the box - iou loss used for w and h\n",
        "    xy_loss = self.mse(pred_boxes[..., :2], target_boxes[..., :2])\n",
        "\n",
        "    # 2 : w and h loss (iou loss)\n",
        "    wh_loss = self.iou_loss(pred_boxes, target_boxes)\n",
        "\n",
        "    # 3 : object confidence\n",
        "    conf_loss = self.bce(pred_conf, target_conf)\n",
        "\n",
        "    # 4 : class predictions\n",
        "    # class_loss = self.ce(pred_classes, target_classes.argmax(-1))\n",
        "\n",
        "    total_loss = xy_loss + wh_loss + conf_loss #+ class_loss\n",
        "    return total_loss\n",
        "\n",
        "    # intersection under curve\n",
        "    def iou_loss(self, pred_boxes, target_boxes):\n",
        "\n",
        "      pred_x, pred_y, pred_w, pred_h = pred_boxes[..., 0], pred_boxes[..., 1], pred_boxes[..., 2], pred_boxes[..., 3]\n",
        "      target_x, target_y, target_w, target_h = target_boxes[..., 0],target_boxes[..., 1],target_boxes[..., 2],target_boxes[..., 3]\n",
        "\n",
        "      # convert to w/h to min/max coords\n",
        "      pred_x1 = pred_x - pred_w / 2\n",
        "      pred_y1 = pred_y - pred_h / 2\n",
        "      pred_x2 = pred_x + pred_w / 2\n",
        "      pred_y2 = pred_y + pred_h / 2\n",
        "\n",
        "      target_x1 = target_x - target_w / 2\n",
        "      target_y1 = target_y - target_h / 2\n",
        "      target_x2 = target_x + target_w / 2\n",
        "      target_y2 = target_y + target_h / 2\n",
        "\n",
        "      # get intersection area\n",
        "      inter_x1 = torch.max(pred_x1, target_x1)\n",
        "      inter_y1 = torch.max(pred_y1, target_y1)\n",
        "      inter_x2 = torch.max(pred_x2, target_x2)\n",
        "      inter_y2 = torch.max(pred_y2, target_y2)\n",
        "\n",
        "      inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n",
        "      inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n",
        "      inter_area = inter_w * inter_h\n",
        "\n",
        "      # get union area\n",
        "      pred_area = pred_w * pred_h\n",
        "      target_area = target_w * target_h\n",
        "      union_area = pred_area + target_area - inter_area\n",
        "\n",
        "      iou = inter_area / (union_area + 1e-6)\n",
        "      return iou.mean()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0bwC26Mp8AYX"
      },
      "outputs": [],
      "source": [
        "# model = YOLOv3(num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwOwkMS1OjFx"
      },
      "source": [
        "Non Maximum Suppression\n",
        "\n",
        "We're allowing 3 anchors per cell as per YOLOv3 so there's lots of duplicates to remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jyEZEJZmOjFy"
      },
      "outputs": [],
      "source": [
        "def nonMaxSuppression(predictions, conf_threshold=0.5, iou_threshold=0.4):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    pass\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "I8J5giblOjFy"
      },
      "outputs": [],
      "source": [
        "class YOLODataset(Dataset):\n",
        "  def __init__(self, image_dir, label_dir, img_size, num_classes, transform=None):\n",
        "    self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
        "    self.label_paths = glob.glob(os.path.join(label_dir, '*.txt'))\n",
        "    self.img_size = img_size\n",
        "    self.transform = transform\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def parse_yolo_line(self, line):\n",
        "    fields = line.strip().split()\n",
        "\n",
        "    if len(fields) != 5:\n",
        "      return None\n",
        "\n",
        "    try:\n",
        "      class_id, x,y,w,h = map(float, fields)\n",
        "    except ValueError:\n",
        "      return None\n",
        "\n",
        "    return class_id, x,y,w,h\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = self.image_paths[index]\n",
        "    img = cv2.imread(img_path)\n",
        "    orig_h, orig_w, _ = img.shape\n",
        "\n",
        "    img = img / 255.0\n",
        "    img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "    label_path = self.label_paths[index]\n",
        "    labels = []\n",
        "    with open(label_path, 'r') as f:\n",
        "      for line in f:\n",
        "        parsed = self.parse_yolo_line(line)\n",
        "        if parsed is None:\n",
        "          continue\n",
        "\n",
        "\n",
        "        class_id, x,y,w,h = parsed\n",
        "\n",
        "        row = [class_id, x,y,w,h]\n",
        "        labels.append([class_id,x,y,w,h])\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.float32) if labels else torch.zeros((0,5))\n",
        "\n",
        "    return img, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  '''\n",
        "  Collate the data\n",
        "\n",
        "  Why is this necessary ?\n",
        "  The YOLODatset().__getitem__() method returns a list of tuples (img, labels)\n",
        "  e.g [(img1, lbl1), (img2, lbl2),...]\n",
        "  When we pass this to the torch DataLoader\n",
        "\n",
        "  '''\n",
        "  # # unpack the list of tuples into two tuples images and labels e.g (img1, img2, img3) and (lbl1, lbl2, lbl3)\n",
        "  # images, labels = zip(*batch)\n",
        "\n",
        "  # # each image and label is a tensor of shape = (Channels, Height, Width)\n",
        "  # # stack each of these images into a tensor of shape Batch(number of images passed in inserted at dim=0), Channels, Height, Width\n",
        "  # images = torch.stack(images, dim=0)\n",
        "\n",
        "  # # repeat the process with labels inserting the batch at dim 0\n",
        "  # labels = torch.cat(labels, dim=0)\n",
        "  imgs = []\n",
        "  lbls_list = []\n",
        "  for (img, lbls) in batch:\n",
        "        imgs.append(img)\n",
        "        lbls_list.append(lbls)\n",
        "  imgs = torch.stack(imgs, dim=0)\n",
        "  return imgs, lbls_list\n"
      ],
      "metadata": {
        "id": "LXkAKmf_iIzf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_img_dir = \"/content/Fire-Smoke-Detection-Yolov11-2/train/images\"\n",
        "train_label_dir = \"/content/Fire-Smoke-Detection-Yolov11-2/train/labels\"\n",
        "valid_img_dir = \"/content/Fire-Smoke-Detection-Yolov11-2/valid/images\"\n",
        "valid_label_dir = \"/content/Fire-Smoke-Detection-Yolov11-2/valid/labels\"\n",
        "train_dataset = YOLODataset(image_dir=train_img_dir, label_dir=train_label_dir, img_size=416, num_classes=2)\n",
        "valid_dataset = YOLODataset(image_dir=valid_img_dir, label_dir=valid_label_dir, img_size=416, num_classes=2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for batch_index, (images, labels) in enumerate(train_loader):\n",
        "  print(f\"Batch {batch_index + 1 }\")\n",
        "  print(f\"Labels : {labels}\")\n",
        "  print(f\"Images : {images.shape}\")\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrMUe9qNT-Wg",
        "outputId": "c8cf45b9-7153-4134-8616-5f16067ecfd5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1\n",
            "Labels : [tensor([[0.0000, 0.4872, 0.5183, 0.0284, 0.0593]]), tensor([[0.0000, 0.2989, 0.7197, 0.3693, 0.3877]])]\n",
            "Images : torch.Size([2, 3, 640, 640])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**"
      ],
      "metadata": {
        "id": "zle3YXmZdWW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10"
      ],
      "metadata": {
        "id": "dN1gSy3HdZ4a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "EUIjvFjjon14"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "# clear cuda memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rr-8A9yFpIGR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLOv3(num_classes=2).to('cuda')\n",
        "\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = YOLOLoss(num_classes=2)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0.0\n",
        "  for batch_index, (image, labels) in enumerate(train_loader):\n",
        "\n",
        "    images = images.to('cuda')\n",
        "    for i in range(len(labels)):\n",
        "        labels[i] = labels[i].to('cuda')\n",
        "\n",
        "    out1, _, _ = model(images)\n",
        "\n",
        "    boxes, classes = out1\n",
        "\n",
        "    predictions = torch.cat([boxes, classes], dim=-1)\n",
        "\n",
        "    batch_pred = predictions.view(-1, predictions.shape[-1])\n",
        "\n",
        "    targets = []\n",
        "    for lbl in labels:\n",
        "      targets.append(lbl)\n",
        "\n",
        "    batch_target = torch.cat(targets, dim=0)\n",
        "\n",
        "    loss = loss_fn(batch_pred, batch_target).to('cuda')\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if (batch_index + 1) % 10 == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_index + 1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
        "  avg_loss = total_loss / len(train_loader)\n",
        "  print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zD5ww9nijFGz",
        "outputId": "1c470656-827e-401e-c087-6e90967029a9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a target size (torch.Size([2, 2])) that is different to the input size (torch.Size([2400, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2400) must match the size of tensor b (2) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-444f99e127d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mbatch_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-3040c011658e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 1 : loss for bounding box (x,y coords) i.e the position of the box - iou loss used for w and h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mxy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 2 : w and h loss (iou loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3882\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3884\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2400) must match the size of tensor b (2) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KN7lte7_juG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}